{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous importons les bibliothèques permettant de récupérer les données. La bliothèque requests permet d'envoyer des requêtes à des sites web, bs4 permet d'analyser et extraire des données de documents HTML et donc de sites internet, re permet de rechercher des expressions régulières (regex) dans du texte, pandas permet de manipuler des bases de données et rapidfuzz fournit des outils pour comparer des chaînes de caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Requirements : pip install bs4 lxml rapidfuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche tout d'abord à confectionner une table avec les informations sur le livre et le numéro d'index auquel il correspond.\n",
    "La cellule ci-dessous permet de récupérer le texte de la page \"gutindex.all\", qui associe à chaque oouvrage un numéro d'index. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL du fichier d'index des textes\n",
    "url_liste_textes = \"https://www.gutenberg.org/dirs/GUTINDEX.ALL.iso-8859-1.txt\"\n",
    "\n",
    "# Téléchargement du fichier d'index\n",
    "request_liste_textes = requests.get(url_liste_textes).content\n",
    "page = bs4.BeautifulSoup(request_liste_textes, \"lxml\")\n",
    "body = page.find(\"body\")\n",
    "index_texte = body.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toujours dans l'optique de créer une table avec les informations sur le texte dans une colonne et l'index dans l'autre, on ne souhaite garder que le texte correspondant aux informations et à l'index. \n",
    "On utilise pour ce faire les balises de début et de fin de l'index puis on supprime les quelques lignes inutiles qui donnent des informations sur le contenu de l'index après avoir converti le texte en une liste de lignes pour faciliter le traitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chercher les indices des marqueurs \"<===LISTINGS===>\" et \"<==End of GUTINDEX.ALL==>\"\n",
    "start_marker = \"<===LISTINGS===>\"\n",
    "end_marker = \"<==End of GUTINDEX.ALL==>\"\n",
    "start_index = index_texte.find(start_marker)\n",
    "end_index = index_texte.find(end_marker)\n",
    "\n",
    "# Extraire le texte entre les marqueurs\n",
    "texte_extrait = index_texte[start_index + len(start_marker):end_index].strip()\n",
    "texte_extrait_lignes = texte_extrait.splitlines()\n",
    "\n",
    "# Filtrer les lignes pertinentes\n",
    "texte_extrait_lignes_trie = texte_extrait_lignes[10:len(texte_extrait_lignes)-1]\n",
    "texte_complet = '\\n'.join(texte_extrait_lignes_trie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nous faut donc maintenant séparer le texte en deux parties, l'une contenant les informations sur l'oeuvre (titre, auteur, date, langue de l'oeuvre...) et l'autre le numéro d'index.\n",
    "La cellule ci-dessous, après avoir divisé le texte en oeuvres, créé une liste d'oeuvre avec description (contenant les informations sur l'oeuvre) et index séparés. La première utilisation du regex dans la boucle permet d'extraire l'index tandis que la deuxième permet de retirer l'index et les espaces superflus pour ne garder que le texte pour la colonne \"Description\". On convertit enfin la liste en dataframe pour la manipule tplus efficacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser le texte en oeuvres\n",
    "oeuvres = re.split(r'(?=\\n{2,})', texte_complet.strip())\n",
    "\n",
    "# Extraire les descriptions et indices\n",
    "# Liste pour stocker les données extraites\n",
    "data = []\n",
    "for oeuvre in oeuvres:\n",
    "    # Trouver l'index dans l'oeuvre\n",
    "    match_index = re.search(r'(?<=\\s\\s)([\\d]+?[A-Z]?)(?=\\n)', oeuvre)\n",
    "    index = match_index.group(1) if match_index else None\n",
    "\n",
    "    # Nettoyer le texte de l'oeuvre\n",
    "    description = re.sub(r'(?<=\\s\\s)([\\d]+?[A-Z]?)(?=\\n)', '', oeuvre).strip()\n",
    "\n",
    "    # Ajouter les données\n",
    "    data.append({\"Description\": description, \"Index\": index})\n",
    "\n",
    "# Convertir les données en DataFrame\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on charge  la base qui contient les noms des livres et les thèmes que leur a attribué l'API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger la base CSV avec les livres pour ajouter l'index\n",
    "books2 = \"final_list\"  # Nom du fichier d'entrée\n",
    "base_csv = pd.read_csv(books2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut maintenant faire correspondre les oeuvres de la base donnée par l'API, c'est-à-dire les oeuvres enrichies des thèmes, avec celles de la base qui contienne leur index de manière à pouvoir aisément récupérer les textes.\n",
    "La cellule ci-dessous commence par conserver la colonne \"Description\" pour la mettre dans la nouvelle base car elle sera utile par la suite. Ensuite, on parcourt en parallèle le titre et l'auteur (pour ne pas confondre des oeuvres éponymes). On nettoie les données puis on définit différents manières de faire correspondre le titre à une partie de la description. En effet, sans cela, on perd de nombreux textes en raison de caractères spéciaux ou de sous-titres présents ou non. On définit donc un match comme une situation où un des trois modes de correspondance du titre et de la description est validé et où l'auteur est identique dans les deux bases. On forme une nouvelle base formée des informations sur les textes, de leur thème  et de leur index. On supprime enfin les lignes sans index puisqu'elles ne permettront pas de récupérer de  texte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserver la colonne 'Description' dans la base initiale\n",
    "base_csv['Description'] = \"\"\n",
    "indices = []\n",
    "\n",
    "# Parcourir les titres et auteurs en parallèle\n",
    "for title, author in zip(base_csv['Title'], base_csv['Author']):\n",
    "    # Nettoyage des données : normalisation\n",
    "    df['Description_clean'] = df['Description'].str.strip().str.lower()\n",
    "    title_clean = title.strip().lower()\n",
    "    author_clean = author.strip().lower()\n",
    "\n",
    "    # Correspondances exactes et approximatives\n",
    "    df['Exact_Match'] = df['Description_clean'].str.match(rf\"^{re.escape(title_clean)}(\\s|[.,;!?]|$)\", na=False)\n",
    "    title_words = title_clean.split()\n",
    "    df['Description_start'] = df['Description_clean'].str.split().str[:len(title_words)].str.join(' ')\n",
    "    df['Starts_With_Title'] = df['Description_start'] == title_clean\n",
    "    df['Similarity'] = df['Description_clean'].apply(lambda x: fuzz.ratio(title_clean, x[:len(title_clean)]))\n",
    "    similarity_threshold = 90\n",
    "    df['Approx_Match'] = df['Similarity'] > similarity_threshold\n",
    "    df['Author_Present'] = df['Description_clean'].str.contains(author_clean, na=False)\n",
    "\n",
    "    # Fusionner les critères\n",
    "    df['Final_Match'] = (df['Exact_Match'] | df['Starts_With_Title'] | df['Approx_Match']) & df['Author_Present']\n",
    "    match = df[df['Final_Match']]\n",
    "\n",
    "    if not match.empty:\n",
    "        # Ajouter l'index et la description correspondants\n",
    "        indices.append(match.iloc[0]['Index'])\n",
    "        base_csv.loc[base_csv['Title'] == title, 'Description'] = match.iloc[0]['Description']\n",
    "    else:\n",
    "        indices.append(None)\n",
    "\n",
    "# Ajouter les indices trouvés à la base\n",
    "base_csv['Index'] = indices\n",
    "\n",
    "# Supprimer les lignes sans index trouvé\n",
    "base_csv_clean = base_csv.dropna(subset=['Index'])\n",
    "\n",
    "# Sauvegarder la base mise à jour\n",
    "base_csv_clean.to_csv(\"base_csv_avec_index.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut enfin réaliser la base de données qui nous intéresse. Elle contient les informations sur les oeuvres, leur thème et leur texte.\n",
    "Pour ce faire, on itère  sur le titre et l'index pour télécharger le texte du livre (l'URL est standardisée et permet donc cette opération). On vérifie à chaque fois le succès du téléchargement pour éviter les erreurs. On se sert des marqueurs présents dans le texte pour enlever ce qui est superflu. Enfin, on supprime les textes qui ne sont pas en français (nos modèles ne traitent que les textes en français), les lignes pour lesquelles il n'y a pas de texte et les lignes pour lesquelles le type n'est pas le bon (des fichiers audios étaient contenus dans la base). On enregistre enfin la base au format csv pour pouvoir nous en servir plus aisément. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger la base nettoyée pour ajouter les textes\n",
    "base_csv_index = pd.read_csv(\"base_csv_avec_index.csv\")\n",
    "base_csv_index['Texte'] = \"\"\n",
    "\n",
    "# Télécharger les textes des livres\n",
    "for livre, index in base_csv_index[['Title', 'Index']].itertuples(index=False):\n",
    "    url = f\"https://www.gutenberg.org/cache/epub/{index}/pg{index}-images.html\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "        page_text = soup.get_text()\n",
    "\n",
    "        start_marker = f\"*** START OF THE PROJECT GUTENBERG EBOOK \"\n",
    "        end_marker = f\"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "        start_index = page_text.find(start_marker)\n",
    "        end_index = page_text.find(end_marker)\n",
    "\n",
    "        if start_index != -1 and end_index != -1:\n",
    "            texte_extrait = page_text[start_index + len(start_marker):end_index].strip()\n",
    "            base_csv_index.loc[base_csv_index['Title'] == livre, 'Texte'] = texte_extrait\n",
    "        else:\n",
    "            print(f\"Marqueurs non trouvés pour {livre} (Index {index}).\")\n",
    "    else:\n",
    "        print(f\"Erreur lors du téléchargement de la page pour {livre} (Index {index}).\")\n",
    "\n",
    "base_csv_index = base_csv_index[base_csv_index[\"Description\"].str.contains(r\"\\[Language: French\\]\", na=False)]\n",
    "base_csv_index = base_csv_index.dropna(subset=['Texte'])\n",
    "base_csv_index = base_csv_index[~base_csv_index['Description'].str.contains('Audio', na=False)]\n",
    "\n",
    "# Sauvegarder la base finale\n",
    "base_csv_index.to_csv(\"Data/base_csv_final.csv\", index=False)\n",
    "print(base_csv_index.head())\n",
    "print(base_csv_index.shape[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
