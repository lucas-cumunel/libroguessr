{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projet Python 2A\n",
    "Arnaud BARRAT • Lucas CUMUNEL • Aloys GALLO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous importons les bibliothèques permettant de récupérer les données. La bliothèque requests permet d'envoyer des requêtes à des sites web, bs4 permet d'analyser et extraire des données de documents HTML et donc de sites internet, re permet de rechercher des expressions régulières (regex) dans du texte, pandas permet de manipuler des bases de données et rapidfuzz fournit des outils pour comparer des chaînes de caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "from langdetect import detect\n",
    "from fuzzywuzzy import fuzz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche tout d'abord à confectionner une table avec les informations sur le livre et le numéro d'index auquel il correspond. Cette base sera ensuite donnée à l'API pour qu'elle donne un ou plusieurs thème à chaque texte. \n",
    "La cellule ci-dessous permet de récupérer le texte de la page \"gutindex.all\", qui associe à chaque oouvrage un numéro d'index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL du fichier d'index des textes\n",
    "url_liste_textes = \"https://www.gutenberg.org/dirs/GUTINDEX.ALL.iso-8859-1.txt\"\n",
    "\n",
    "# Téléchargement du fichier d'index\n",
    "request_liste_textes = requests.get(url_liste_textes).content\n",
    "page = bs4.BeautifulSoup(request_liste_textes, \"lxml\")\n",
    "body = page.find(\"body\")\n",
    "index_texte = body.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toujours dans l'optique de créer une table avec les informations sur le texte dans une colonne et l'index dans l'autre, on ne souhaite garder que le texte correspondant aux informations et à l'index. \n",
    "On utilise pour ce faire les balises de début et de fin de l'index puis on supprime les quelques lignes inutiles qui donnent des informations sur le contenu de l'index après avoir converti le texte en une liste de lignes pour faciliter le traitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chercher les indices des marqueurs \"<===LISTINGS===>\" et \"<==End of GUTINDEX.ALL==>\"\n",
    "start_marker = \"<===LISTINGS===>\"\n",
    "end_marker = \"<==End of GUTINDEX.ALL==>\"\n",
    "start_index = index_texte.find(start_marker)\n",
    "end_index = index_texte.find(end_marker)\n",
    "\n",
    "# Extraire le texte entre les marqueurs\n",
    "texte_extrait = index_texte[start_index + len(start_marker):end_index].strip()\n",
    "texte_extrait_lignes = texte_extrait.splitlines()\n",
    "\n",
    "# Filtrer les lignes pertinentes\n",
    "texte_extrait_lignes_trie = texte_extrait_lignes[10:len(texte_extrait_lignes)-1]\n",
    "texte_complet = '\\n'.join(texte_extrait_lignes_trie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nous faut donc maintenant séparer le texte en deux parties, l'une contenant les informations sur l'oeuvre (titre, auteur, date, langue de l'oeuvre...) et l'autre le numéro d'index.\n",
    "La cellule ci-dessous, après avoir divisé le texte en oeuvres, créé une liste d'oeuvre avec description (contenant les informations sur l'oeuvre) et index séparés. La première utilisation du regex dans la boucle permet d'extraire l'index tandis que la deuxième permet de retirer l'index et les espaces superflus pour ne garder que le texte pour la colonne \"Description\". On convertit enfin la liste en dataframe pour la manipuler plus efficacement et on ne garde que les textes en français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser le texte en oeuvres\n",
    "oeuvres = re.split(r'(?=\\n{2,})', texte_complet.strip())\n",
    "\n",
    "# Extraire les descriptions et indices\n",
    "# Liste pour stocker les données extraites\n",
    "data = []\n",
    "for oeuvre in oeuvres:\n",
    "    # Trouver l'index dans l'oeuvre\n",
    "    match_index = re.search(r'(?<=\\s\\s)([\\d]+?[A-Z]?)(?=\\n)', oeuvre)\n",
    "    index = match_index.group(1) if match_index else None\n",
    "\n",
    "    # Nettoyer le texte de l'oeuvre\n",
    "    description = re.sub(r'(?<=\\s\\s)([\\d]+?[A-Z]?)(?=\\n)', '', oeuvre).strip()\n",
    "\n",
    "    # Ajouter les données\n",
    "    data.append({\"Description\": description, \"Index\": index})\n",
    "\n",
    "# Convertir les données en DataFrame\n",
    "df_livres = pd.DataFrame(data)\n",
    "df_livres_fr = df_livres[df_livres[\"Description\"].str.contains(r\"\\[Language: French\\]\", na=False)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin que l'API nous donne bien des thèmes pour les textes envoyés, nous avons fait le choix, après quellques essais, de nous restreindre à ceux d'auteurs célèbres. Nous avons donc réalisé une liste d'auteurs célèbres des 17\n",
    "La cellule ci-dessous ne garde que les ouvrages dont l'auteur est dans la liste. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of French Writers abritrarily defined and chosen in the 17th, 18th and 19th century\n",
    "auteurs = [\n",
    "    # 17th century\n",
    "    \"Honoré d'Urfé\", \"Madeleine de Scudéry\", \"Paul Scarron\", \"Jean de La Fontaine\",\n",
    "    \"Madame de Lafayette\", \"Charles Sorel\", \"Tristan L'Hermite\", \"François de Salignac de La Mothe-Fénelon\",\n",
    "    \"Savinien de Cyrano de Bergerac\",\n",
    "    \n",
    "    # 18th century\n",
    "    \"Montesquieu\", \"Voltaire\", \"Jean-Jacques Rousseau\", \"Denis Diderot\", \"Marivaux\",\n",
    "    \"Abbé Prévost\", \"Pierre Choderlos de Laclos\", \"Beaumarchais\", \n",
    "    \n",
    "    # 19th century\n",
    "    \"Honoré de Balzac\", \"Victor Hugo\", \"Alexandre Dumas\", \"Gustave Flaubert\", \"Émile Zola\",\n",
    "    \"Stendhal\", \"Alfred de Musset\", \"George Sand\", \"Jules Verne\", \"Alphonse Daudet\",\n",
    "    \"Théophile Gautier\", \"Edmond de Goncourt\",\n",
    "    \"Joris-Karl Huysmans\", \"Octave Mirbeau\", \n",
    "    \"Prosper Mérimée\", \"Eugène Sue\", \"Charles Nodier\",\n",
    "    \"Gaston Leroux\", \"François-René de Chateaubriand\", \"Anatole France\", \"Gustave Flaubert\", \"Alfred Jarry\",\n",
    "    \"Guy de Maupassant\", \"Romain Rolland\", \"Alfred Séguin\", \"Alfred de Vigny\", \"Paul de Kock\"\n",
    "\n",
    "]\n",
    "\n",
    "#On créé une expression réulière que signifie \"ou\" pour l'utiliser ensuite\n",
    "auteurs_join = \"|\".join(map(re.escape, auteurs))\n",
    "# Filtrer les lignes qui contiennent au moins un des auteurs\n",
    "df_livres_fr_filtré = df_livres_fr[df_livres_fr[\"Description\"].str.contains(auteurs_join, na=False)]\n",
    "df_livres_fr_filtré.to_csv(\"livres_fr_triés.csv\", index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des thèmes des livres de ces auteurs et traitement de ces thèmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des livres dont on dispose des thèmes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut maintenant faire correspondre les oeuvres de la base donnée par l'API, c'est-à-dire les oeuvres enrichies des thèmes, avec celles de la base qui contienne leur index de manière à pouvoir aisément récupérer les textes.\n",
    "La cellule ci-dessous commence par conserver la colonne \"Description\" pour la mettre dans la nouvelle base car elle sera utile par la suite. Ensuite, on parcourt en parallèle le titre et l'auteur (pour ne pas confondre des oeuvres éponymes). On nettoie les données puis on définit différents manières de faire correspondre le titre à une partie de la description. En effet, sans cela, on perd de nombreux textes en raison de caractères spéciaux ou de sous-titres présents ou non. On définit donc un match comme une situation où un des trois modes de correspondance du titre et de la description est validé et où l'auteur est identique dans les deux bases. On forme une nouvelle base formée des informations sur les textes, de leur thème  et de leur index. On supprime enfin les lignes sans index puisqu'elles ne permettront pas de récupérer de  texte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserver la colonne 'Description' dans la base initiale\n",
    "base_csv['Description'] = \"\"\n",
    "indices = []\n",
    "\n",
    "# Parcourir les titres et auteurs en parallèle\n",
    "for title, author in zip(base_csv['Title'], base_csv['Author']):\n",
    "    # Nettoyage des données : normalisation\n",
    "    df['Description_clean'] = df['Description'].str.strip().str.lower()\n",
    "    title_clean = title.strip().lower()\n",
    "    author_clean = author.strip().lower()\n",
    "\n",
    "    # Correspondances exactes et approximatives\n",
    "    df['Exact_Match'] = df['Description_clean'].str.match(rf\"^{re.escape(title_clean)}(\\s|[.,;!?]|$)\", na=False)\n",
    "    title_words = title_clean.split()\n",
    "    df['Description_start'] = df['Description_clean'].str.split().str[:len(title_words)].str.join(' ')\n",
    "    df['Starts_With_Title'] = df['Description_start'] == title_clean\n",
    "    df['Similarity'] = df['Description_clean'].apply(lambda x: fuzz.ratio(title_clean, x[:len(title_clean)]))\n",
    "    similarity_threshold = 90\n",
    "    df['Approx_Match'] = df['Similarity'] > similarity_threshold\n",
    "    df['Author_Present'] = df['Description_clean'].str.contains(author_clean, na=False)\n",
    "\n",
    "    # Fusionner les critères\n",
    "    df['Final_Match'] = (df['Exact_Match'] | df['Starts_With_Title'] | df['Approx_Match']) & df['Author_Present']\n",
    "    match = df[df['Final_Match']]\n",
    "\n",
    "    if not match.empty:\n",
    "        # Ajouter l'index et la description correspondants\n",
    "        indices.append(match.iloc[0]['Index'])\n",
    "        base_csv.loc[base_csv['Title'] == title, 'Description'] = match.iloc[0]['Description']\n",
    "    else:\n",
    "        indices.append(None)\n",
    "\n",
    "# Ajouter les indices trouvés à la base\n",
    "base_csv['Index'] = indices\n",
    "\n",
    "# Supprimer les lignes sans index trouvé\n",
    "base_csv_clean = base_csv.dropna(subset=['Index'])\n",
    "\n",
    "# Sauvegarder la base mise à jour\n",
    "base_csv_clean.to_csv(\"base_csv_avec_index.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut enfin réaliser la base de données qui nous intéresse. Elle contient les informations sur les oeuvres, leur thème et leur texte.\n",
    "Pour ce faire, on itère  sur le titre et l'index pour télécharger le texte du livre (l'URL est standardisée et permet donc cette opération). On vérifie à chaque fois le succès du téléchargement pour éviter les erreurs. On se sert des marqueurs présents dans le texte pour enlever ce qui est superflu. Enfin, on supprime les textes qui ne sont pas en français (nos modèles ne traitent que les textes en français), les lignes pour lesquelles il n'y a pas de texte et les lignes pour lesquelles le type n'est pas le bon (des fichiers audios étaient contenus dans la base). On enregistre enfin la base au format csv pour pouvoir nous en servir plus aisément. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger la base nettoyée pour ajouter les textes\n",
    "base_csv_index = pd.read_csv(\"base_csv_avec_index.csv\")\n",
    "base_csv_index['Texte'] = \"\"\n",
    "\n",
    "# Télécharger les textes des livres\n",
    "for livre, index in base_csv_index[['Title', 'Index']].itertuples(index=False):\n",
    "    url = f\"https://www.gutenberg.org/cache/epub/{index}/pg{index}-images.html\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "        page_text = soup.get_text()\n",
    "\n",
    "        start_marker = f\"*** START OF THE PROJECT GUTENBERG EBOOK \"\n",
    "        end_marker = f\"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "        start_index = page_text.find(start_marker)\n",
    "        end_index = page_text.find(end_marker)\n",
    "\n",
    "        if start_index != -1 and end_index != -1:\n",
    "            texte_extrait = page_text[start_index + len(start_marker):end_index].strip()\n",
    "            base_csv_index.loc[base_csv_index['Title'] == livre, 'Texte'] = texte_extrait\n",
    "        else:\n",
    "            print(f\"Marqueurs non trouvés pour {livre} (Index {index}).\")\n",
    "    else:\n",
    "        print(f\"Erreur lors du téléchargement de la page pour {livre} (Index {index}).\")\n",
    "\n",
    "base_csv_index = base_csv_index[base_csv_index[\"Description\"].str.contains(r\"\\[Language: French\\]\", na=False)]\n",
    "base_csv_index = base_csv_index.dropna(subset=['Texte'])\n",
    "base_csv_index = base_csv_index[~base_csv_index['Description'].str.contains('Audio', na=False)]\n",
    "\n",
    "# Sauvegarder la base finale\n",
    "base_csv_index.to_csv(\"Data/base_csv_final.csv\", index=False)\n",
    "print(base_csv_index.head())\n",
    "print(base_csv_index.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deuxième étape de l'extraction des données : Trouver les thèmes des livres disponibles sur Gutenberg\n",
    "\n",
    "La deuxième étape de l'extraction des données consiste à trouver les thèmes des livres dont on sait qu'on dispose des textes dans Gutenberg. Cependant, comme Gutenberg ne fournit pas cette information, nous avons utilisé l'API **OpenLibrary**, qui est l'une des rares API regroupant les thèmes des livres. \n",
    "\n",
    "#### Sélection des auteurs\n",
    "Pour optimiser l'extraction, nous avons défini une liste d'auteurs à partir des pages Wikipedia recensant les romanciers français, en particulier ceux mentionnés dans la [catégorie des romanciers français par siècle](https://fr.wikipedia.org/wiki/Cat%C3%A9gorie:Romancier_fran%C3%A7ais_par_si%C3%A8cle). Nous avons croisé cette liste avec nos connaissances en prépa B/L pour garder uniquement les auteurs dits \"classiques\". Cette approche présente trois avantages :\n",
    "1. **Maximiser les chances de trouver les thèmes des livres** : Les auteurs classiques ont écrit des ouvrages qui sont souvent cités et reconnus, augmentant ainsi les probabilités d’obtenir des informations sur leurs thèmes.\n",
    "2. **Réduire la taille de la base de données** : En limitant la sélection aux auteurs classiques, nous évitons une requête trop large qui pourrait être difficile à traiter en termes de temps et qui aurait un impact environnemental plus fort.\n",
    "3. **Faciliter l'application du projet** : En se concentrant sur des auteurs classiques, les livres sont non seulement largement accessibles, mais également facilement retrouvables dans la vie réelle pour des applications pratiques.\n",
    "\n",
    "Toutes les étapes détaillées du processus sont expliquées dans le notebook [Get_themes.ipynb](#........................#).\n",
    "\n",
    "#### Nettoyage des données brutes\n",
    "Nous partons de la liste des livres (fichier `nom_fichier_csv`) pour lesquels nous savons que les textes complets sont disponibles. Après avoir récupéré ces données brutes, nous effectuons un nettoyage pour obtenir un dataframe regroupant les noms des auteurs et les titres des livres. Voici quelques étapes du nettoyage :\n",
    "1. **Suppression des numéros de tomes apparaissant dans les titres ;**\n",
    "2. **Suppression des livres en doublon ;** \n",
    "3. **Suppression des caractères indésirables (accents mal encodés, symboles inutiles), que nous supprimons pour standardiser les titres.**\n",
    "\n",
    "#### Requête à l'API OpenLibrary\n",
    "Une fois les données nettoyées, nous interrogeons l'API **OpenLibrary** pour obtenir les thèmes associés à chaque livre. \n",
    "\n",
    "#### Raisons du choix de l'API OpenLibrary\n",
    "L'API OpenLibrary a été choisie car elle est gratuite, facile d'accès, et ne nécessite pas de clé d'API. De plus, elle contient des informations sur les thèmes des livres, ce qui est essentiel pour l'analyse de nos données.\n",
    "\n",
    "#### Problème de langue des thèmes\n",
    "L'API OpenLibrary renvoie les thèmes dans différentes langues, mais jamais en français. C’est pourquoi nous avons utilisé l'API **Lingva** (https://lingva.ml/) pour traduire ces thèmes en français.\n",
    "\n",
    "#### Raisons du choix de l'API Lingva\n",
    "L'API **Lingva** a été choisie car elle est gratuite, ne nécessite pas de clé API et permet de traduire les thèmes rapidement. Cette API nous a permis de résoudre le problème de la langue des thèmes obtenus, en les traduisant automatiquement en français.\n",
    "\n",
    "#### Mapping des thèmes\n",
    "Une fois les thèmes traduits, nous avons réalisé un **mapping manuel** pour nettoyer les données :\n",
    "- **Suppression des thèmes non pertinents** : Certains thèmes comme \"Langue française\" ou \"Littérature classique\" étaient inutiles pour l'analyse ;\n",
    "- **Regroupement des thèmes similaires pour créer des catégories plus générales.**\n",
    "\n",
    "#### Réduction de la taille des données\n",
    "L'un des principaux défis rencontrés dans ce projet était la taille des données. En effet, nous avons commencé avec un total de 476 livres, mais il était nécessaire de réduire cette quantité pour faciliter les analyses suivantes. Le nettoyage des données a permis d'atteindre un total de 276 livres, mais ce nombre de livre demeurait trop important compte tenu de la puissance de calcul dont nous disposons.\n",
    "\n",
    "Nous avons donc :\n",
    "1. **Examiné la fréquence d'apparition des auteurs** : Nous avons limité le nombre de livres par auteur à deux pour éviter d'avoir une base de données trop biaisée.\n",
    "2. Après cette réduction, nous avons obtenu une base de données finale de **96 livres**.\n",
    "\n",
    "#### Difficultés rencontrées\n",
    "Au cours de ces étapes, plusieurs difficultés ont été rencontrées :\n",
    "1. **Trouver des API pertinentes et gratuites** : trouver des API qui fournissaient les informations dont on avait besoin a été une tâche complexe.\n",
    "2. **Traductions incomplètes** : L'API ne parvenait pas toujours à traduire l'ensemble des thèmes, même en insérant des pauses via `time.sleep()` avec des durées de plus en plus longues ou en mettant uniquement en langue source 'en' pour spécifier que tous les textes sont initialement en anglais.\n",
    "\n",
    " >**Remarque** : Étant donné que la requête de deux API prend chacune plus de 30 minutes, soit un total d'environ une heure, on importe uniquement la base de données après le nettoyage pour voir le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = pd.read_csv('final_list')\n",
    "df_books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistiques descriptives des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering des livres : Application de la méthode VBGMM\n",
    "\n",
    "Pour réaliser un clustering sur les livres, nous avons choisi d'appliquer la méthode **VBGMM (Variational Bayesian Gaussian Mixture Model)**.\n",
    "\n",
    "#### Pourquoi la méthode VBGMM ?\n",
    "1. **Estimation du nombre de clusters** :  \n",
    "   Contrairement à une méthode classique de mélange gaussien (GMM), où le nombre de clusters doit être fixé à l'avance, VBGMM utilise une approche bayésienne variationnelle pour déterminer automatiquement le nombre optimal de clusters en fonction des données.\n",
    "\n",
    "2. **Gestion des données complexes** :  \n",
    "   La méthode est particulièrement efficace pour les ensembles de données complexes et de haute dimension, comme ceux impliquant des thèmes littéraires et des métadonnées textuelles.\n",
    "\n",
    "3. **Réduction des biais** :  \n",
    "   La pondération bayésienne permet d'éviter le sur-ajustement en pénalisant les clusters inutiles ou redondants.\n",
    "\n",
    "\n",
    "### Conditions pour utiliser la méthode VBGMM\n",
    "\n",
    "L'application de la méthode **VBGMM (Variational Bayesian Gaussian Mixture Model)** repose sur certaines conditions et prérequis pour garantir des résultats optimaux et une interprétation valide des clusters.\n",
    "\n",
    "#### 1. Données numériques dans un espace vectoriel\n",
    "- Les données d'entrée doivent être représentées sous forme numérique. Nous utilisons donc une matrice Tf-IDF dense, c'est-à-dire dont la majorité des éléments sont différents de zéro.\n",
    "\n",
    "#### 2. Hypothèse d'une distribution gaussienne\n",
    "- VBGMM suppose que chaque cluster suit une distribution gaussienne.  \n",
    "\n",
    "\n",
    "#### 3. Absence de valeurs aberrantes\n",
    "- Les données ne doivent pas contenir de valeurs aberrantes importantes comme des NaN ou des infinis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/base_csv_final.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Tf_Idf (lemmas) :\n",
    "    \n",
    "    voc=[]\n",
    "    for l in lemmas :\n",
    "        \n",
    "        voc.extend(l['Lemmes'].tolist())  # Assuming 'Lemmes' column contains the terms\n",
    "    voc = list(set(voc)) \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(lowercase=False, vocabulary=voc, min_df=2)\n",
    "    documents = [\" \".join(l['Lemmes'].tolist()) for l in lemmas]\n",
    "    vectorizer.fit(documents)\n",
    "    vectors=vectorizer.transform(documents)\n",
    "    \"\"\"\n",
    "    vectors=[]\n",
    "    for l in lemmas :\n",
    "        X=vectorizer.transform([\" \".join(l['Lemmes'].tolist())])\n",
    "        vectors.append(X)\n",
    "        \"\"\"\n",
    "    return vectors,vectorizer\n",
    "t_lemmas=[v for k, v in pd.read_parquet('Data/lemmes.parquet', engine='pyarrow').groupby('Id')]\n",
    "vec,vectorizer=Tf_Idf(t_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir chaque élément de la liste en tableau dense\n",
    "vec_dense = [matrix.toarray() for matrix in vec]\n",
    "\n",
    "# Combiner toutes les matrices en une seule\n",
    "vec_combined = np.vstack(vec_dense)\n",
    "\n",
    "# Vérification de la structure des données \n",
    "# Vérifier que la moyenne des variances des colonnes n'est pas nulle\n",
    "print(\"Moyenne des variances des colonnes :\", np.var(vec_combined, axis=0).mean()) \n",
    "\n",
    "# Vérifier s'il n'y a pas des NaN\n",
    "print(\"Existence de Nan :\", np.any(np.isnan(vec_combined)))  \n",
    "\n",
    "# Vérifier s'il n'y a pas des infinis\n",
    "print(\"Existence de valeurs infinis :\", np.any(np.isinf(vec_combined)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vérification des hypothèses et réduction de dimension\n",
    "\n",
    "Les hypothèses **1** et **3** sont bien vérifiées. Il est maintenant nécessaire de vérifier si les données suivent une **distribution gaussienne**.\n",
    "\n",
    "Pour ce faire, une **Analyse en Composantes Principales (ACP)** est réalisée. Cette méthode permet de **réduire la dimension des données** tout en préservant le maximum de variance. Elle facilite également la **projection des données** dans un espace de dimension réduite, rendant leur visualisation plus intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer PCA pour réduire à 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(vec_combined)\n",
    "\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1])\n",
    "plt.title(\"PCA Projection of TF-IDF Data\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation de la répartition des données\n",
    "\n",
    "Les données semblent être réparties de manière **uniforme autour de l'origine**. Visuellement, cela signifie qu'elles ne présentent pas de **concentration particulière** dans certaines zones de l'espace étudié. \n",
    "\n",
    "Cette répartition pourrait suggérer qu'il n'y a pas de **structures sous-jacentes** ou de **tendances marquées**, comme des regroupements naturels de points, qui pourraient indiquer la présence de clusters. \n",
    "\n",
    "Cependant, il est important de noter que l'absence de **clusters évidents a priori** ne signifie pas nécessairement qu'il n'en existe pas.\n",
    "\n",
    "Pour approfondir cette analyse, affichons l'**histogramme de la première et de la seconde composante principale** afin d'examiner la structure des données de manière plus détaillée.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des sous-graphiques côte à côte\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Histogramme de la première composante principale\n",
    "sns.histplot(reduced_data[:, 0], kde=True, ax=axes[0])\n",
    "axes[0].set_title('Histogramme de la première composante principale')\n",
    "\n",
    "# Histogramme de la seconde composante principale\n",
    "sns.histplot(reduced_data[:, 1], kde=True, ax=axes[1])\n",
    "axes[1].set_title('Histogramme de la seconde composante principale')\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des distributions des composantes principales\n",
    "\n",
    "On observe que les données sont **globalement centrées en 0**, avec des distributions qui semblent suivre des **lois normales**, sans présenter de **valeurs aberrantes** ou d'**asymétries** notables.\n",
    "\n",
    "Cette première observation suggère que les données pourraient suivre une **distribution normale**. Cependant, pour confirmer cette hypothèse de manière plus rigoureuse, il est nécessaire de procéder à une **évaluation précise**.\n",
    "\n",
    "Pour ce faire, nous utiliserons les **QQ plots** (quantile-quantile plots), qui permettent de comparer les **quantiles des données** avec ceux d'une distribution normale. Cette méthode aide à valider l'**hypothèse de normalité** des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des sous-graphiques côte à côte\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Q-Q plot de la première composante principale\n",
    "stats.probplot(reduced_data[:, 0], dist=\"norm\", plot=axes[0])\n",
    "axes[0].set_title('Q-Q Plot de la première composante principale')\n",
    "\n",
    "# Q-Q plot de la seconde composante principale\n",
    "stats.probplot(reduced_data[:, 1], dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot de la seconde composante principale')\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation de la normalité avec les QQ plots\n",
    "\n",
    "Hormis pour les **valeurs aux extrémités**, les points des **QQ plots** suivent de manière précise la **diagonale** correspondant à la distribution théorique de la **loi normale**. \n",
    "\n",
    "Cette observation indique que les **quantiles des données** s'alignent étroitement avec ceux d'une distribution normale, confirmant ainsi la normalité des données.\n",
    "\n",
    "Ces résultats permettent d'affirmer, avec beaucoup de confiance, que la **distribution des données suit une loi normale**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorov-Smirnov test for normality\n",
    "stat1, p1 = kstest(reduced_data[:, 0], 'norm', args=(reduced_data[:, 0].mean(), reduced_data[:, 0].std()))\n",
    "stat2, p2 = kstest(reduced_data[:, 1], 'norm', args=(reduced_data[:, 1].mean(), reduced_data[:, 1].std()))\n",
    "print(f'Kolmogorov-Smirnov Test for First Principal Component: Statistics={stat1}, p-value={p1}')\n",
    "print(f'Kolmogorov-Smirnov Test for Second Principal Component: Statistics={stat2}, p-value={p2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **test de Kolmogorov-Smirnov** (KS) est un test statistique non paramétrique utilisé pour comparer la distribution empirique des données avec une distribution théorique donnée.\n",
    "\n",
    "## Hypothèses du test\n",
    "\n",
    "- **Hypothèse nulle (H₀)** : Les données suivent la distribution théorique spécifiée.  \n",
    "\n",
    "\n",
    "- **Hypothèse alternative (H₁)** : Les données ne suivent pas la distribution théorique spécifiée.  \n",
    "\n",
    "## Analyse de la p-valeur\n",
    "\n",
    "Les **p-valeurs** sont ici très élevées. Cela signifie que nous ne rejetons pas l'hypothèse nulle et que nous pouvons conclure que les données suivent bien la distribution théorique avec une grande certitude.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "L'hypothèse 3 est ainsi vérifiée. Nous pouvons désormais appliquer la méthode **VBGMM** (Variational Bayesian Gaussian Mixture Model) pour réaliser des clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VBGMM model\n",
    "vbgmm = BayesianGaussianMixture(n_components=6, covariance_type='full')\n",
    "    \n",
    "# Fit the model to the data\n",
    "vbgmm.fit(reduced_data)\n",
    "    \n",
    "# Predict the cluster for each sample\n",
    "labels = vbgmm.predict(reduced_data)\n",
    "    \n",
    "df['Cluster'] = labels\n",
    "\n",
    "# Plot the clusters\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=labels, cmap='viridis')\n",
    "plt.title('Clusters of Books')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VBGMM model\n",
    "vbgmm = BayesianGaussianMixture(n_components=6, covariance_type='full')\n",
    "    \n",
    "# Fit the model to the data\n",
    "vbgmm.fit(reduced_data)\n",
    "    \n",
    "# Predict the cluster for each sample\n",
    "labels = vbgmm.predict(reduced_data)\n",
    "    \n",
    "df['Cluster'] = labels\n",
    "\n",
    "# Plot the clusters\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=labels, cmap='viridis')\n",
    "plt.title('Clusters of Books')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variabilité des clusters lors des exécutions répétées\n",
    "\n",
    "On remarque qu'en appliquant plusieurs fois le même algorithme, les **clusters** obtenus sont modifiés. Cela est probablement dû à l'**initialisation aléatoire** des paramètres du modèle, qui peut influencer les résultats finaux.\n",
    "\n",
    "Pour garantir la **reproductibilité** de l'analyse et éviter les variations liées à cette initialisation, nous allons fixer la **graine aléatoire** (*seed*). Cela permettra d'obtenir des résultats cohérents à chaque exécution de l'algorithme.\n",
    "\n",
    "Poursuivons donc l'analyse en fixant cette graine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed()\n",
    "\n",
    "# Create a VBGMM model\n",
    "vbgmm = BayesianGaussianMixture(n_components=6, covariance_type='full')\n",
    "    \n",
    "# Fit the model to the data\n",
    "vbgmm.fit(reduced_data)\n",
    "    \n",
    "# Predict the cluster for each sample\n",
    "labels = vbgmm.predict(reduced_data)\n",
    "    \n",
    "df['Cluster'] = labels\n",
    "\n",
    "# Plot the clusters\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=labels, cmap='viridis')\n",
    "plt.title('Clusters of Books')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultat de l'algorithme VBGMM\n",
    "\n",
    "L'algorithme **VBGMM (Variational Bayesian Gaussian Mixture Model)** a permis de distinguer **trois clusters** dans les données. Ces clusters représentent des regroupements naturels, identifiés à partir des caractéristiques des points dans l'espace étudié.\n",
    "\n",
    "Avant d'analyser ces clusters en détail pour mieux comprendre leurs spécificités et les thématiques qu'ils regroupent, étudions la qualité des clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les centroïdes des clusters\n",
    "def compute_centroids(X, labels):\n",
    "    centroids = []\n",
    "    for cluster in np.unique(labels):\n",
    "        cluster_points = X[labels == cluster]\n",
    "        centroid = cluster_points.mean(axis=0)\n",
    "        centroids.append(centroid)\n",
    "    return np.array(centroids)\n",
    "\n",
    "# Calcul de la distance intra-cluster (moyenne des distances aux centroïdes)\n",
    "def intra_cluster_distance(X, labels, centroids):\n",
    "    intra_distances = []\n",
    "    for cluster in np.unique(labels):\n",
    "        cluster_points = X[labels == cluster]\n",
    "        centroid = centroids[cluster]\n",
    "        distance = np.mean(np.linalg.norm(cluster_points - centroid, axis=1))\n",
    "        intra_distances.append(distance)\n",
    "    return np.mean(intra_distances)\n",
    "\n",
    "# Calcul de la distance inter-cluster (moyenne des distances entre clusters)\n",
    "def inter_cluster_distance(X, labels, centroids):\n",
    "    inter_distances = []\n",
    "    for i, centroid_i in enumerate(centroids):\n",
    "        for j, centroid_j in enumerate(centroids):\n",
    "            if i != j:\n",
    "                dist = np.linalg.norm(centroid_i - centroid_j)\n",
    "                inter_distances.append(dist)\n",
    "    return np.mean(inter_distances)\n",
    "\n",
    "\n",
    "# Calcul des centroïdes\n",
    "centroids = compute_centroids(df, labels)\n",
    "\n",
    "# Calcul de la distance intra-cluster\n",
    "intra_dist = intra_cluster_distance(df, labels, centroids)\n",
    "\n",
    "# Calcul de la distance inter-cluster\n",
    "inter_dist = inter_cluster_distance(df, labels, centroids)\n",
    "\n",
    "print(f\"Distance intra-cluster : {intra_dist}\")\n",
    "print(f\"Distance inter-cluster : {inter_dist}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation de la qualité du modèle\n",
    "\n",
    "L'évaluation de la qualité du modèle en utilisant les distances inter et intra-clusters n'est pas possible en raison de limitations computationnelles, entraînant l'erreur suivante : **\"Maximum call stack size exceeded\"**. \n",
    "\n",
    "Pour contourner cette contrainte, nous avons décidé d'évaluer les clusters à l'aide du **score de silhouette**. \n",
    "\n",
    "Le **score de silhouette** mesure la cohésion et la séparation des clusters :  \n",
    "- Il calcule la **différence entre la distance moyenne d'un point avec les autres points de son cluster (cohésion)** et la **distance moyenne avec les points du cluster voisin le plus proche (séparation)**.\n",
    "- Un score de silhouette proche de 1 indique que les clusters sont bien définis, c'est-à-dire qu'ils ont une forte cohésion et une bonne séparation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbgmm = BayesianGaussianMixture(n_components=3, random_state=42)\n",
    "vbgmm.fit(reduced_data)\n",
    "labels = vbgmm.predict(reduced_data)\n",
    "\n",
    "# Calcul du silhouette score\n",
    "f\"Silhouette Score : {silhouette_score(reduced_data, labels)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation du score de silhouette\n",
    "\n",
    "Un score de silhouette de **0,42** indique que les clusters formés par l'algorithme sont **relativement pertinents**. Cela suggère que les points d'un même cluster sont modérément proches les uns des autres, tandis qu'ils sont bien séparés des autres clusters.\n",
    "\n",
    "### Limitation et alternative\n",
    "\n",
    "Pour améliorer la prise en compte des thèmes, une autre approche aurait été d'encoder les thèmes dans une matrice **one-hot** et de la **concaténer avec la matrice TF-IDF**. Cependant, cette méthode présente un inconvénient majeur :  \n",
    "- **Modification de la distribution des données** : Une telle transformation affecte la structure statistique des données, qui ne suivraient plus une loi gaussienne.  \n",
    "- Cela rendrait l'algorithme **VBGMM caduc**, car celui-ci repose sur l'hypothèse de normalité des données.\n",
    "\n",
    "### Comparaison des thèmes des clusters\n",
    "\n",
    "Pour répondre à la problématique et analyser les différences entre les clusters, nous comparons les thèmes associés à chaque cluster. Une représentation sous forme de **heatmap** est utilisée pour visualiser la fréquence des thèmes dans chaque cluster. Cette approche permet de mettre en lumière les caractéristiques dominantes de chaque cluster et d'explorer les relations entre thèmes et regroupements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre de livres dans le cluster 0 : \", df['Cluster'].value_counts()[0])\n",
    "print(\"Nombre de livres dans le cluster 1 : \", df['Cluster'].value_counts()[1])\n",
    "print(\"Nombre de livres dans le cluster 2 : \", df['Cluster'].value_counts()[2])\n",
    "\n",
    "\n",
    "df['Themes'] = df['Themes'].apply(lambda x: x.split(', ') if isinstance(x, str) else x)\n",
    "\n",
    "df_exploded = df.explode('Themes')\n",
    "theme_counts = df_exploded.groupby(['Cluster', 'Themes']).size().unstack(fill_value=0)\n",
    "\n",
    "# Créer la heatmap pour visualiser les comptes de thèmes par cluster\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(theme_counts, annot=True, cmap='Blues', fmt='d', cbar=True)\n",
    "\n",
    "# Ajouter des titres et labels\n",
    "plt.title('Nombre d\\'apparitions de chaque thème par cluster', fontsize=14)\n",
    "plt.xlabel('Thèmes', fontsize=12)\n",
    "plt.ylabel('Clusters', fontsize=12)\n",
    "\n",
    "# Afficher la heatmap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des clusters\n",
    "\n",
    "### Cluster 0 et Cluster 2 : Similarités et différences\n",
    "On remarque que les clusters 0 et 2 sont relativement similaires, principalement en raison de la forte occurrence des thèmes tels que **décadence**, **roman**, **société et politique**, **fiction historique**, **pouvoir** et **coutumes**. Ces thèmes suggèrent que les livres dans ces deux clusters partagent des préoccupations liées aux dynamiques sociales et politiques, explorant les relations humaines, les conflits de pouvoir, ainsi que les dilemmes moraux dans des contextes historiques.\n",
    "\n",
    "**Cluster 0** se distingue par une présence supplémentaire des thèmes **passions** et **ambitions**. Cela suggère que ce cluster regroupe des romans qui ne se contentent pas de décrire des contextes sociaux ou politiques, mais qui mettent également l'accent sur les émotions humaines et les aspirations personnelles, rendant ces récits plus intenses et dramatiques. Ces livres explorent les enjeux individuels au sein de sociétés marquées par des tensions sociales et politiques, avec une forte dimension historique.\n",
    "\n",
    "**Cluster 2**, bien qu'il partage des thèmes similaires, semble davantage mettre l'accent sur des aspects **institutionnels** et **sociaux**, tels que la **culture**, les **mœurs**, et **l'éducation**. Cela suggère que les livres du cluster 2 se concentrent peut-être plus sur les structures sociales et les valeurs culturelles que sur les dynamiques émotionnelles et les conflits personnels. Ces romans abordent probablement des questionnements sur les normes et les pratiques sociales dans un cadre historique ou social donné, sans nécessairement explorer les passions ou les ambitions des individus.\n",
    "\n",
    "### Cluster 1 : Romans d'aventure et de voyage\n",
    "Le **cluster 1** semble se différencier nettement des autres en mettant l'accent sur les **romans d'aventure** et de **voyage**, souvent caractérisés par une **dimension fantastique** ou **merveilleuse**. Les thèmes présents dans ce cluster suggèrent des récits qui, tout en étant parfois ancrés dans une certaine réalité géographique, s'éloignent dans une certaine mesure des préoccupations sociales ou politiques pour se concentrer sur des aventures extraordinaires, des explorations de nouveaux mondes ou des quêtes héroïques. Ces livres semblent inviter le lecteur à s'évader, en découvrant des univers lointains et fascinants, souvent empreints de mystère ou de fantastique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pistes d'amélioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Projet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mProjet\u001b[49m \n",
      "\u001b[0;31mNameError\u001b[0m: name 'Projet' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
